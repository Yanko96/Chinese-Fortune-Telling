"""
bench_multihop.py
~~~~~~~~~~~~~~~~~
å¤šè·³æ¨ç† Benchmarkï¼šè¯„ä¼° RAG ç³»ç»Ÿå¤„ç†è·¨æ–‡æ¡£æ¨ç†çš„èƒ½åŠ›ã€‚

ä¸æ™®é€š rag_bench.py çš„åŒºåˆ«
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  è¯„ä¼°æŒ‡æ ‡   chain_score = æ¨ç†é“¾æ­¥éª¤è¦†ç›–ç‡ï¼ˆè€Œé RAGASï¼‰
  é¢å¤–ç»Ÿè®¡   cross_book_hit = æ£€ç´¢ç»“æœæ˜¯å¦è¦†ç›–äº†å¤šä¸ªæ¥æºä¹¦ç±
  è¾“å…¥       benchmarks/qa_multihop.jsonï¼ˆç”± generate_qa_multihop.py ç”Ÿæˆï¼‰
  è¾“å‡º       benchmarks/results/multihop/<timestamp>/
               results.json     å®Œæ•´é€é¢˜å¾—åˆ†
               summary.json     æ±‡æ€»å¯¹æ¯”è¡¨
               report.md        å¯è¯»æŠ¥å‘Š

chain_score çš„è®¡ç®—
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  å¯¹æ¯é“é¢˜ï¼šç»™ Kimi çœ‹æ¨ç†é“¾ + æ¨¡å‹ç­”æ¡ˆï¼Œé€æ­¥æ‰“åˆ†ï¼ˆ1/0 åˆ†ï¼‰
  chain_score = sum(step_scores) / len(reasoning_chain)
  é¢å¤–ç»´åº¦ï¼š
    hop_ok_rate   chain_score â‰¥ 0.6 çš„é¢˜ç›®å æ¯”ï¼ˆ"åŸºæœ¬æ¨å¯¹"ï¼‰
    full_ok_rate  chain_score = 1.0 çš„é¢˜ç›®å æ¯”ï¼ˆ"å®Œå…¨æ¨å¯¹"ï¼‰
    cross_hit     æ£€ç´¢æ–‡æ¡£è¦†ç›–ä¸¤ä¸ªæ¥æºä¹¦ç±çš„å æ¯”

ç”¨æ³•ç¤ºä¾‹
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  cd E:\\repos\\Chinese-Fortune-Telling

  # è¯„ä¼°å•ä¸ªé…ç½®
  python scripts/bench_multihop.py \\
      --configs configs/rag/v5/hyde_rerank_topn7.yaml \\
      --dataset benchmarks/qa_multihop.json

  # å¯¹æ¯”å¤šä¸ªé…ç½®
  python scripts/bench_multihop.py \\
      --configs configs/rag/v2/hybrid.yaml configs/rag/v5/hyde_rerank_topn7.yaml \\
      --dataset benchmarks/qa_multihop.json \\
      --max-samples 30

  # ä»…æ‰“åˆ†ï¼Œè·³è¿‡ RAGï¼ˆç”¨å·²æœ‰ç­”æ¡ˆæ–‡ä»¶ï¼‰
  python scripts/bench_multihop.py --score-only --answers-file benchmarks/results/multihop/.../answers.json
"""

from __future__ import annotations

import argparse
import json
import os
import re
import sys
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime
from pathlib import Path
from typing import Any

import yaml

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

sys.path.insert(0, str(Path(__file__).parent.parent / "api"))
sys.path.insert(0, str(Path(__file__).parent.parent / "scripts"))

from dotenv import load_dotenv

load_dotenv(override=True)

# â”€â”€ è¶…å‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EVAL_MODEL        = "moonshot-v1-32k"
GEN_MODEL         = "moonshot-v1-32k"    # ç”Ÿæˆ RAG ç­”æ¡ˆç”¨
MAX_WORKERS       = 3
DEFAULT_OUTPUT    = "benchmarks/results/multihop"

# â”€â”€ è¯„ä¼° Prompt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CHAIN_EVAL_PROMPT = """\
ä½ æ˜¯ä¸€åä¸­å›½å‘½ç†å­¦è¯„å·ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹å‘½ç†å¤šè·³æ¨ç†é¢˜çš„æ¨¡å‹ç­”æ¡ˆã€‚

ã€é—®é¢˜ã€‘
{question}

ã€æ ‡å‡†æ¨ç†é“¾ã€‘
{reasoning_chain}

ã€æ¨¡å‹ç­”æ¡ˆã€‘
{answer}

è¯„åˆ†æ ‡å‡†
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è¯·é€æ­¥åˆ¤æ–­æ¨¡å‹ç­”æ¡ˆæ˜¯å¦"å®è´¨æ€§æ¶µç›–"äº†æ¨ç†é“¾çš„æ¯ä¸ªæ­¥éª¤ã€‚
  1 åˆ† = ç­”æ¡ˆæ˜ç¡®æåˆ°è¯¥æ­¥éª¤çš„æ ¸å¿ƒå‘½é¢˜ï¼ˆå…è®¸æ¢è¡¨è¿°ï¼Œä½†å†…å®¹ç­‰ä»·ï¼‰
  0 åˆ† = ç­”æ¡ˆæœªæåŠï¼Œæˆ–å†…å®¹æ˜æ˜¾é”™è¯¯

åªè¾“å‡ºä»¥ä¸‹ JSONï¼Œä¸åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š
{{"step_scores": [1, 0, 1, ...], "chain_score": 0.67, "comment": "..."}}
"""

RAG_ANSWER_PROMPT = """\
ä½ æ˜¯ä¸­å›½ä¼ ç»Ÿå‘½ç†å­¦ç ”ç©¶è€…ï¼Œç²¾é€šã€Šä¸‰å‘½é€šä¼šã€‹ã€Šå­å¹³çœŸè¯ ã€‹ã€Šæ»´å¤©é«“ã€‹ç­‰å¤å…¸å‘½ç†æ–‡çŒ®ã€‚
è¯·ä¸¥æ ¼ä¾æ®ä¸‹æ–¹æä¾›çš„å¤ç±åŸæ–‡ï¼Œç”¨ä¸­æ–‡ç›´æ¥ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä½œç­”è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»æœ‰åŸæ–‡ä¾æ®
2. ä¼˜å…ˆå¼•ç”¨å…·ä½“æœ¯è¯­ã€æ ¼å±€åç§°
3. å›ç­”ç›´æ¥é’ˆå¯¹é—®é¢˜ï¼Œä¸è¦è®²åºŸè¯
4. å¿…é¡»è¿è´¯æ¨ç†ï¼Œä¸è¦åªæŠ„å½•åŸæ–‡

å‚è€ƒå¤ç±åŸæ–‡ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}
"""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RAG è°ƒç”¨ï¼ˆå¤ç”¨ rag_bench.py çš„æ ¸å¿ƒå‡½æ•°ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_rag_bench():
    """æ‡’åŠ è½½ rag_bench çš„ build_retriever + build_vector_store"""
    bench_dir = Path(__file__).parent
    if str(bench_dir) not in sys.path:
        sys.path.insert(0, str(bench_dir))
    import rag_bench
    return rag_bench


def _with_heartbeat(label: str, fn):
    """Run fn() and print a simple start/done message. No threads (threads
    can propagate KeyboardInterrupt into heavy C-extension calls on Windows)."""
    print(f"  {label}â€¦", flush=True)
    result = fn()
    return result


def build_retriever_from_config(cfg_path: str):
    """ä» YAML é…ç½®æ–‡ä»¶æ„é€ æ£€ç´¢å™¨ï¼Œè¿”å› (retriever, config_dict)ã€‚

    å¤ç”¨ rag_bench.build_retriever + chroma_utils.get_vectorstoreï¼Œ
    ä¸ run_benchmark() ä¸­çš„åˆå§‹åŒ–é€»è¾‘ä¿æŒä¸€è‡´ã€‚
    """
    rb = load_rag_bench()

    with open(cfg_path, encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    chroma_dir      = cfg.get("chroma_dir", "./chroma_db_bge")
    embedding_model = cfg.get("embedding_model", "BAAI/bge-small-zh-v1.5")
    os.environ["CHROMA_DIR"]      = chroma_dir
    os.environ["EMBEDDING_MODEL"] = embedding_model

    # é‡ç½® chroma_utils å…¨å±€ç¼“å­˜ï¼Œä¿è¯åˆ‡æ¢é…ç½®æ—¶ä¸å¤ç”¨æ—§ vectorstore
    import chroma_utils as _cu
    _cu._vectorstore        = None
    _cu._embedding_function = None

    from chroma_utils import get_vectorstore
    vs = _with_heartbeat("[2/3] åŠ è½½ embedding æ¨¡å‹", get_vectorstore)
    print(f"\r  [2/3] å‘é‡å­˜å‚¨å°±ç»ª âœ“ (count={vs._collection.count()})        ", flush=True)

    retriever = _with_heartbeat("[3/3] æ„å»ºæ£€ç´¢å™¨ï¼ˆé¦–æ¬¡åŠ è½½ BGE æ¨¡å‹ï¼‰", lambda: rb.build_retriever(cfg, vs))
    print(f"\r  [3/3] æ£€ç´¢å™¨å°±ç»ª âœ“                                           ", flush=True)
    return retriever, cfg


def retrieve_and_answer(
    question: str,
    retriever,
    kimi_client,
) -> tuple[str, list[dict]]:
    """ç”¨æ£€ç´¢å™¨æ‹‰ä¸Šä¸‹æ–‡ï¼Œè°ƒ Kimi ç”Ÿæˆç­”æ¡ˆã€‚è¿”å› (answer, docs)"""
    docs = []
    for attempt in range(3):
        try:
            docs = retriever.invoke(question)
            break
        except Exception as e:
            if attempt == 2:
                # æ£€ç´¢å½»åº•å¤±è´¥ï¼Œè¿”å›ç©º
                return f"[retrieve_error: {e}]", []
            time.sleep(3 * (attempt + 1))
            try:
                docs = retriever.get_relevant_documents(question)
                break
            except Exception:
                time.sleep(3 * (attempt + 1))

    context_parts = []
    doc_metas = []
    for doc in docs:
        context_parts.append(doc.page_content)
        doc_metas.append({
            "content": doc.page_content[:200],
            "book": (doc.metadata or {}).get("book", ""),
        })

    context = "\n\n---\n\n".join(context_parts)
    prompt = RAG_ANSWER_PROMPT.format(context=context, question=question)

    try:
        resp = kimi_client.chat.completions.create(
            model=GEN_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=600,
            timeout=60,
        )
        answer = resp.choices[0].message.content.strip()
    except Exception as e:
        answer = f"[api_error: {e}]"
    return answer, doc_metas


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Chain-score è¯„ä¼°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def evaluate_chain(
    question: str,
    reasoning_chain: list[str],
    answer: str,
    kimi_client,
    max_retries: int = 2,
) -> dict:
    """å¯¹å•é“é¢˜æ‰“ chain_scoreã€‚è¿”å› {step_scores, chain_score, comment}"""
    chain_str = "\n".join(f"{i+1}. {step}" for i, step in enumerate(reasoning_chain))
    prompt = CHAIN_EVAL_PROMPT.format(
        question=question,
        reasoning_chain=chain_str,
        answer=answer,
    )
    for attempt in range(max_retries + 1):
        try:
            resp = kimi_client.chat.completions.create(
                model=EVAL_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=300,
                timeout=60,
            )
            raw = resp.choices[0].message.content.strip()
            m = re.search(r"\{[\s\S]*?\}", raw)
            if not m:
                raise ValueError(f"No JSON found: {raw[:100]}")
            data = json.loads(m.group())
            scores = data.get("step_scores", [])
            # æ ¡éªŒ
            if not scores or not all(s in (0, 1, 0.5) for s in scores):
                raise ValueError(f"Bad step_scores: {scores}")
            chain_score = sum(scores) / len(reasoning_chain)
            return {
                "step_scores":  scores,
                "chain_score":  round(chain_score, 4),
                "comment":      data.get("comment", ""),
            }
        except Exception as e:
            if attempt == max_retries:
                return {"step_scores": [], "chain_score": 0.0, "comment": f"eval_error: {e}"}
            time.sleep(2)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å•é¢˜å¤„ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_single_item(
    item: dict,
    retriever,
    kimi_client,
    cfg_name: str,
) -> dict:
    """æ‰§è¡Œå•é¢˜ï¼šæ£€ç´¢ + ç”Ÿæˆ + chain_score è¯„ä¼°"""
    t0 = time.perf_counter()
    question        = item["question"]
    reasoning_chain = item["reasoning_chain"]
    required_hops   = item.get("required_hops", len(reasoning_chain))
    source_books    = {item["metadata"]["book1"], item["metadata"]["book2"]}

    try:
        answer, doc_metas = retrieve_and_answer(question, retriever, kimi_client)
    except Exception as e:
        answer = f"[retrieve_fatal: {e}]"
        doc_metas = []
    t_retrieve = time.perf_counter() - t0

    # cross_book_hit: æ£€ç´¢åˆ°çš„æ–‡æ¡£æ˜¯å¦è¦†ç›–äº†ä¸¤ä¸ªæ¥æºä¹¦ç±
    retrieved_books = {d["book"] for d in doc_metas if d["book"]}
    cross_hit = len(source_books & retrieved_books) == len(source_books)

    # chain-score
    eval_result = evaluate_chain(question, reasoning_chain, answer, kimi_client)

    total_time = time.perf_counter() - t0
    return {
        "id":             item.get("id", ""),
        "config":         cfg_name,
        "question":       question,
        "answer":         answer,
        "golden_answer":  item.get("golden_answer", ""),
        "reasoning_chain": reasoning_chain,
        "step_scores":    eval_result["step_scores"],
        "chain_score":    eval_result["chain_score"],
        "comment":        eval_result["comment"],
        "required_hops":  required_hops,
        "cross_book_hit": cross_hit,
        "retrieved_books": list(retrieved_books),
        "source_books":   list(source_books),
        "metadata":       item.get("metadata", {}),
        "latency_s":      round(total_time, 2),
        "retrieve_s":     round(t_retrieve, 2),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ±‡æ€»ç»Ÿè®¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def summarize_results(results: list[dict], cfg_name: str) -> dict:
    if not results:
        return {"config": cfg_name, "n": 0}

    n               = len(results)
    chain_scores    = [r["chain_score"] for r in results]
    cross_hits      = [r["cross_book_hit"] for r in results]
    latencies       = sorted(r["latency_s"] for r in results)

    hop_ok          = sum(1 for s in chain_scores if s >= 0.6)
    full_ok         = sum(1 for s in chain_scores if s >= 0.99)

    # æŒ‰ required_hops åˆ†ç»„
    by_hops: dict[int, list[float]] = {}
    for r in results:
        h = r.get("required_hops", 0)
        by_hops.setdefault(h, []).append(r["chain_score"])

    by_hops_avg = {k: round(sum(v) / len(v), 4) for k, v in sorted(by_hops.items())}

    # æŒ‰ä¹¦ç±å¯¹åˆ†ç»„
    by_pair: dict[str, list[float]] = {}
    for r in results:
        key = f"{r['metadata'].get('book1','?')}Ã—{r['metadata'].get('book2','?')}"
        by_pair.setdefault(key, []).append(r["chain_score"])

    by_pair_avg = {k: round(sum(v) / len(v), 4) for k, v in sorted(by_pair.items())}

    return {
        "config":        cfg_name,
        "n":             n,
        "chain_score_mean":  round(sum(chain_scores) / n, 4),
        "chain_score_min":   round(min(chain_scores), 4),
        "chain_score_max":   round(max(chain_scores), 4),
        "hop_ok_rate":       round(hop_ok / n, 4),   # chain_score â‰¥ 0.6
        "full_ok_rate":      round(full_ok / n, 4),  # chain_score = 1.0
        "cross_book_hit_rate": round(sum(cross_hits) / n, 4),
        "latency_p50":       round(latencies[n // 2], 2),
        "latency_p90":       round(latencies[int(n * 0.9)], 2),
        "by_required_hops":  by_hops_avg,
        "by_book_pair":      by_pair_avg,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¯¹æ¯”è¡¨æ‰“å°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def print_comparison(summaries: list[dict]) -> None:
    if not summaries:
        return

    cols = [
        ("Config",         "config",             20),
        ("N",              "n",                   4),
        ("chain_mean",     "chain_score_mean",    10),
        ("hop_okâ‰¥0.6",     "hop_ok_rate",         10),
        ("full_ok=1.0",    "full_ok_rate",        10),
        ("cross_hit",      "cross_book_hit_rate", 10),
        ("p50_lat",        "latency_p50",          8),
    ]

    header = "  ".join(f"{h:<{w}}" for h, _, w in cols)
    sep    = "  ".join("-" * w for _, _, w in cols)
    print("\n" + "=" * len(header))
    print("  Multi-hop RAG Benchmark Results")
    print("=" * len(header))
    print(header)
    print(sep)

    for s in summaries:
        row = "  ".join(
            f"{str(s.get(k, '')):<{w}}"
            for _, k, w in cols
        )
        print(row)

    print("=" * len(header))

    # è¯¦ç»†ï¼šæŒ‰ required_hops ç»†åˆ†
    print("\n  chain_score by required_hops:")
    all_hop_keys = sorted({
        hk
        for s in summaries
        for hk in s.get("by_required_hops", {})
    })
    hop_header = f"  {'Config':<20}" + "".join(f"  {h}hop" for h in all_hop_keys)
    print(hop_header)
    for s in summaries:
        hop_vals = "".join(
            f"  {s['by_required_hops'].get(h, 'N/A'):<5}"
            for h in all_hop_keys
        )
        print(f"  {s['config']:<20}{hop_vals}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Markdown æŠ¥å‘Š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def write_report(
    summaries: list[dict],
    all_results: dict[str, list[dict]],
    output_dir: Path,
    dataset_path: str,
) -> None:
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    lines = [
        "# Multi-hop RAG Benchmark Report",
        f"\nç”Ÿæˆæ—¶é—´ï¼š{now_str}",
        f"\næ•°æ®é›†ï¼š{dataset_path}",
        f"\næ ·æœ¬æ•°ï¼š{summaries[0]['n'] if summaries else 0}",
        "\n## æ±‡æ€»å¯¹æ¯”\n",
        "| Config | chain_mean | hop_okâ‰¥0.6 | full_ok=1.0 | cross_hit | p50 lat |",
        "|--------|-----------|-----------|------------|----------|---------|",
    ]
    for s in summaries:
        lines.append(
            f"| {s['config']} | {s['chain_score_mean']} | {s['hop_ok_rate']}"
            f" | {s['full_ok_rate']} | {s['cross_book_hit_rate']} | {s['latency_p50']}s |"
        )

    lines.append("\n## chain_score æŒ‰æ¨ç†æ­¥éª¤æ•°ç»†åˆ†\n")
    all_hop_keys = sorted({hk for s in summaries for hk in s.get("by_required_hops", {})})
    hop_header = "| Config | " + " | ".join(f"{h}hop" for h in all_hop_keys) + " |"
    lines.append(hop_header)
    lines.append("|" + "--------|" * (len(all_hop_keys) + 1))
    for s in summaries:
        vals = " | ".join(str(s["by_required_hops"].get(h, "N/A")) for h in all_hop_keys)
        lines.append(f"| {s['config']} | {vals} |")

    lines.append("\n## chain_score æŒ‰ä¹¦ç±å¯¹ç»†åˆ†\n")
    all_pair_keys = sorted({pk for s in summaries for pk in s.get("by_book_pair", {})})
    if all_pair_keys:
        pair_header = "| Config | " + " | ".join(all_pair_keys) + " |"
        lines.append(pair_header)
        lines.append("|" + "--------|" * (len(all_pair_keys) + 1))
        for s in summaries:
            pvals = " | ".join(str(s["by_book_pair"].get(p, "N/A")) for p in all_pair_keys)
            lines.append(f"| {s['config']} | {pvals} |")

    # åº•éƒ¨è¿½åŠ é€é¢˜è¯¦æƒ…ï¼ˆç¬¬ä¸€ä¸ªé…ç½®ï¼‰
    if all_results:
        cfg0 = list(all_results.keys())[0]
        lines.append(f"\n## é€é¢˜å¾—åˆ†ï¼ˆ{cfg0}ï¼‰\n")
        lines.append("| # | chain | hops | books | question (truncated) |")
        lines.append("|---|-------|------|-------|----------------------|")
        for i, r in enumerate(all_results[cfg0], 1):
            lines.append(
                f"| {i} | {r['chain_score']} | {r['required_hops']}"
                f" | {r['metadata'].get('book1','?')}Ã—{r['metadata'].get('book2','?')}"
                f" | {r['question'][:40]}â€¦ |"
            )

    report_path = output_dir / "report.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))
    print(f"  Report written â†’ {report_path}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    # Force UTF-8 unbuffered output so Chinese chars don't crash file redirects
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8", line_buffering=True)
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding="utf-8", line_buffering=True)

    parser = argparse.ArgumentParser(description="Multi-hop RAG Benchmark")
    parser.add_argument(
        "--configs", nargs="+", required=True,
        help="YAML config files for RAG systems to compare",
    )
    parser.add_argument(
        "--dataset", default="benchmarks/qa_multihop.json",
        help="Multi-hop QA dataset (JSON)",
    )
    parser.add_argument(
        "--max-samples", type=int, default=None,
        help="Limit number of questions (for quick tests)",
    )
    parser.add_argument(
        "--output-dir", default=DEFAULT_OUTPUT,
        help="Directory to write result files",
    )
    parser.add_argument(
        "--no-parallel", action="store_true",
        help="Disable concurrent evaluation (easier debugging)",
    )
    args = parser.parse_args()

    root = Path(__file__).parent.parent
    os.chdir(root)

    # â”€â”€ API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    from openai import OpenAI

    api_key = os.environ.get("KIMI_API_KEY")
    if not api_key:
        raise EnvironmentError("KIMI_API_KEY not set in .env")
    kimi = OpenAI(api_key=api_key, base_url="https://api.moonshot.cn/v1", timeout=90.0)

    # â”€â”€ åŠ è½½æ•°æ®é›† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    dataset_path = args.dataset
    with open(dataset_path, encoding="utf-8") as f:
        dataset: list[dict] = json.load(f)

    if args.max_samples:
        dataset = dataset[: args.max_samples]

    print(f"Loaded {len(dataset)} multi-hop QA samples from {dataset_path}")
    hop_dist: dict[int, int] = {}
    for item in dataset:
        h = item.get("required_hops", len(item.get("reasoning_chain", [])))
        hop_dist[h] = hop_dist.get(h, 0) + 1
    print(f"  required_hops distribution: { {k: hop_dist[k] for k in sorted(hop_dist)} }")

    # â”€â”€ åˆ›å»ºè¾“å‡ºç›®å½• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = Path(args.output_dir) / ts
    out_dir.mkdir(parents=True, exist_ok=True)
    print(f"  Output â†’ {out_dir}")

    # â”€â”€ é€é…ç½®è¿è¡Œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_results:  dict[str, list[dict]] = {}
    all_summaries: list[dict]           = []

    for cfg_path in args.configs:
        cfg_name = Path(cfg_path).stem
        print(f"\n{'â•'*60}")
        print(f"  Config: {cfg_name}  ({cfg_path})")
        print(f"{'â•'*60}")

        try:
            retriever, cfg = build_retriever_from_config(cfg_path)
        except Exception as exc:
            import traceback as _tb
            print(f"  ERROR building retriever: {exc}")
            _tb.print_exc()
            continue
        results: list[dict] = []

        workers = 1 if args.no_parallel else MAX_WORKERS

        if workers == 1:
            checkpoint_file = out_dir / f"checkpoint_{cfg_name}.jsonl"
            _ckpt_seq = open(checkpoint_file, "w", encoding="utf-8")
            # å¿½ç•¥ SIGINTï¼ˆCtrl+C / Windows é—ç•™ä¿¡å·ï¼‰ï¼Œé˜²æ­¢ KI æ‰“æ–­å•é¢˜å¾ªç¯
            import signal as _signal
            _orig_sigint = _signal.signal(_signal.SIGINT, _signal.SIG_IGN)
            for i, item in enumerate(dataset, 1):
                try:
                    r = run_single_item(item, retriever, kimi, cfg_name)
                except (Exception, KeyboardInterrupt) as exc:
                    import traceback as _tb
                    print(f"\n  ERROR on item {i}: {type(exc).__name__}: {exc}", flush=True)
                    if not isinstance(exc, KeyboardInterrupt):
                        _tb.print_exc()
                    r = {
                        "id": item.get("id", ""), "config": cfg_name,
                        "question": item["question"], "answer": f"[fatal: {exc}]",
                        "golden_answer": item.get("golden_answer", ""),
                        "reasoning_chain": item["reasoning_chain"],
                        "step_scores": [], "chain_score": 0.0,
                        "comment": f"fatal_error: {type(exc).__name__}",
                        "required_hops": item.get("required_hops", 3),
                        "cross_book_hit": False, "retrieved_books": [],
                        "source_books": [item["metadata"]["book1"], item["metadata"]["book2"]],
                        "metadata": item.get("metadata", {}),
                        "latency_s": 0.0, "retrieve_s": 0.0,
                    }
                results.append(r)
                _ckpt_seq.write(json.dumps(r, ensure_ascii=False) + "\n")
                _ckpt_seq.flush()
                print(
                    f"  [{i:3d}/{len(dataset)}] chain={r['chain_score']:.3f}"
                    f"  cross={int(r['cross_book_hit'])}  lat={r['latency_s']:.1f}s"
                    f"  | {r['question'][:30]}â€¦",
                    flush=True,
                )
            _ckpt_seq.close()
            _signal.signal(_signal.SIGINT, _orig_sigint)  # æ¢å¤ä¿¡å·å¤„ç†
        else:
            lock = threading.Lock()
            done = [0]
            checkpoint_file = out_dir / f"checkpoint_{cfg_name}.jsonl"
            _ckpt_f = open(checkpoint_file, "a", encoding="utf-8")

            def _run(item):
                try:
                    r = run_single_item(item, retriever, kimi_client, cfg_name)
                except Exception as exc:
                    r = {
                        "id":             item.get("id", ""),
                        "config":         cfg_name,
                        "question":       item["question"],
                        "answer":         f"[fatal: {exc}]",
                        "golden_answer":  item.get("golden_answer", ""),
                        "reasoning_chain": item["reasoning_chain"],
                        "step_scores":    [],
                        "chain_score":    0.0,
                        "comment":        f"fatal_error: {exc}",
                        "required_hops":  item.get("required_hops", 3),
                        "cross_book_hit": False,
                        "retrieved_books": [],
                        "source_books":   [item["metadata"]["book1"], item["metadata"]["book2"]],
                        "metadata":       item.get("metadata", {}),
                        "latency_s":      0.0,
                        "retrieve_s":     0.0,
                    }
                with lock:
                    done[0] += 1
                    print(
                        f"  [{done[0]:3d}/{len(dataset)}] chain={r['chain_score']:.3f}"
                        f"  cross={int(r['cross_book_hit'])}  lat={r['latency_s']:.1f}s"
                        f"  | {r['question'][:30]}â€¦"
                    )
                    _ckpt_f.write(json.dumps(r, ensure_ascii=False) + "\n")
                    _ckpt_f.flush()
                return r

            with ThreadPoolExecutor(max_workers=workers) as pool:
                results = list(pool.map(_run, dataset))
            _ckpt_f.close()

        all_results[cfg_name] = results

        # ä¿å­˜é€é¢˜ç»“æœ
        results_file = out_dir / f"results_{cfg_name}.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        summary = summarize_results(results, cfg_name)
        all_summaries.append(summary)
        print(
            f"\n  chain_score: mean={summary['chain_score_mean']}  "
            f"hop_okâ‰¥0.6={summary['hop_ok_rate']}  "
            f"full_ok=1.0={summary['full_ok_rate']}\n"
            f"  cross_hit={summary['cross_book_hit_rate']}  "
            f"p50={summary['latency_p50']}s"
        )

    # â”€â”€ æ±‡æ€» & æŠ¥å‘Š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print_comparison(all_summaries)

    # ä¿å­˜æ±‡æ€» JSON
    summary_file = out_dir / "summary.json"
    with open(summary_file, "w", encoding="utf-8") as f:
        json.dump(all_summaries, f, ensure_ascii=False, indent=2)
    print(f"\n  Summary â†’ {summary_file}")

    # Markdown æŠ¥å‘Š
    write_report(all_summaries, all_results, out_dir, dataset_path)

    # â”€â”€ è§£è¯»æç¤º â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if all_summaries:
        best = max(all_summaries, key=lambda s: s["chain_score_mean"])
        print(f"\nğŸ†  Best config: {best['config']}  chain_score={best['chain_score_mean']}")

    print(
        f"\nğŸ’¡  Graph RAG æ”¹è¿›ç›®æ ‡ï¼šchain_score æ¯”æœ€ä½³ Flat RAG baseline æå‡ â‰¥ 0.15\n"
        f"    ï¼ˆå°¤å…¶æ˜¯ required_hopsâ‰¥3 çš„é¢˜ç›®ï¼‰\n"
    )


if __name__ == "__main__":
    import traceback as _root_tb
    try:
        main()
    except BaseException as _root_exc:
        with open("bench_crash_log.txt", "w", encoding="utf-8") as _cf:
            _root_tb.print_exc(file=_cf)
            _cf.write(f"\nCrash type: {type(_root_exc).__name__}: {_root_exc}\n")
        _root_tb.print_exc()
        raise
