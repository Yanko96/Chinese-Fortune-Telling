"""
build_index_bge.py
~~~~~~~~~~~~~~~~~~
ç”¨è¯­ä¹‰è¾¹ç•Œåˆ†å— + ä¸­æ–‡ BGE åµŒå…¥ï¼Œé‡æ–°æ„å»ºå‘é‡ç´¢å¼•ã€‚

åˆ†å—ç­–ç•¥ï¼ˆæŒ‰ä¹¦ç›®ï¼‰:
  ä¸‰å‘½é€šä¼š : æŒ‰ â—‹ æ ‡è®°åˆ‡åˆ†ï¼ˆ298 å¤„ï¼‰ï¼Œæ¯èŠ‚ä¸ºä¸€å®Œæ•´è¯­ä¹‰å•å…ƒ
  å­å¹³çœŸè¯  : æŒ‰æ±‰å­—ç« èŠ‚åºå·ï¼ˆåå…«ã€â€¦ï¼‰åˆ‡åˆ†ï¼ˆ94 ç« ï¼‰
  æ»´å¤©é«“   : æŒ‰ç©ºè¡Œï¼ˆ\\n\\nï¼‰åˆ‡åˆ†ï¼Œå°†å››å­—è¯€ + åŸæ³¨ + ä»»æ°æ›° åˆå¹¶ä¸ºä¸€ä¸ªè¯­ä¹‰å—

äºŒæ¬¡åˆ†å‰²ï¼šè¶…è¿‡ MAX_CHUNK_CHARS çš„å—ç”¨ RecursiveCharacterTextSplitter å†åˆ‡ï¼›
åˆå¹¶å°å—ï¼šçŸ­äº MIN_CHUNK_CHARS çš„å—åˆå¹¶åˆ°ä¸‹ä¸€å—ã€‚

è¾“å‡º: chroma_db_bge/  (collection: "langchain")
åµŒå…¥: BAAI/bge-small-zh-v1.5  (384-dimï¼Œä¸­æ–‡ä¼˜åŒ–)

ç”¨æ³•:
    cd E:\\repos\\Chinese-Fortune-Telling
    E:\\Software\\Anaconda3\\envs\\rag\\python.exe scripts/build_index_bge.py
"""

from __future__ import annotations

import re
import sys
from pathlib import Path

# â”€â”€ è¶…å‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BOOKS = [
    {
        "path": "fortune_books/san_ming_tong_hui.pdf",
        "label": "ä¸‰å‘½é€šä¼š",
        "splitter": "circle_marker",   # æŒ‰ â—‹ åˆ‡åˆ†
    },
    {
        "path": "fortune_books/di_tian_sui.pdf",
        "label": "æ»´å¤©é«“",
        "splitter": "blank_line",      # æŒ‰ç©ºè¡Œ + åˆå¹¶å››å­—è¯€æ®µè½
    },
    {
        "path": "fortune_books/zi_ping_zhen_quan.pdf",
        "label": "å­å¹³çœŸè¯ ",
        "splitter": "chapter_number",  # æŒ‰æ±‰å­—ç« èŠ‚å·åˆ‡åˆ†
    },
]

CHROMA_DIR     = "./chroma_db_bge"
EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
MAX_CHUNK_CHARS = 1000   # è¶…å‡ºåˆ™äºŒæ¬¡åˆ‡åˆ†
MIN_CHUNK_CHARS = 50     # ä½äºæ­¤åˆ™åˆå¹¶åˆ°ä¸‹ä¸€å—
COLLECTION_NAME = "langchain"

# â”€â”€ é¡µçœ‰æ¸…ç†è§„åˆ™ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

HEADER_PATTERNS = [
    r"ä¸‰å‘½é€šä¼š\s*Â·\s*\d+\s*Â·",          # ä¸‰å‘½é€šä¼š Â·9Â·
    r"æ»´å¤©é«“\s*Â·\s*\d+\s*Â·",            # æ»´å¤©é«“ Â·3Â·
    r"-\s*\d+\s*/\s*\d+\s*-",           # -9/153-  (å­å¹³çœŸè¯ )
    r"å­å¹³çœŸè¯ [^\n]{0,20}Â·\s*\d+\s*Â·",  # å­å¹³çœŸè¯ -æ²ˆå­ç» Â·5Â·
]


def clean_text(raw: str) -> str:
    """å»æ‰é¡µçœ‰ã€è§„èŒƒåŒ–è¿ç»­ç©ºè¡Œï¼ˆä¸‰è¡Œä»¥ä¸Šå‹ç¼©ä¸ºä¸¤è¡Œï¼‰ã€‚"""
    for pat in HEADER_PATTERNS:
        raw = re.sub(pat, "", raw)
    raw = re.sub(r"\n{3,}", "\n\n", raw)
    return raw.strip()


def extract_full_text(pdf_path: str) -> str:
    """æ‹¼æ¥ PDF æ‰€æœ‰é¡µçš„æ–‡æœ¬ã€‚"""
    from pypdf import PdfReader
    reader = PdfReader(pdf_path)
    pages = []
    for page in reader.pages:
        t = page.extract_text() or ""
        pages.append(t)
    return clean_text("\n\n".join(pages))


# â”€â”€ å„ä¹¦ä¸“å±åˆ†å—å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_by_circle_marker(text: str) -> list[str]:
    """
    æŒ‰ â—‹ åˆ‡åˆ†ï¼ˆä¸‰å‘½é€šä¼šï¼‰ã€‚
    â—‹ æ ‡è®°å§‹ç»ˆå‡ºç°åœ¨å°èŠ‚é¦–è¡Œï¼Œå¦‚ã€Œâ—‹è®ºäº”è¡Œç”Ÿæˆã€ï¼Œ
    ç”¨å‰ç»æ–­è¨€ä¿ç•™ â—‹ åœ¨æ¯ä¸ªå—çš„å¼€å¤´ã€‚
    """
    parts = re.split(r"(?=â—‹)", text)
    return [p.strip() for p in parts if p.strip()]


def split_by_chapter_number(text: str) -> list[str]:
    """
    æŒ‰æ±‰å­—ç« èŠ‚å·åˆ‡åˆ†ï¼ˆå­å¹³çœŸè¯ ï¼‰ã€‚
    Pattern: è¡Œé¦– + æ±‰å­—æ•°å­—åºåˆ— + é¡¿å·/å¥å·ï¼Œä¾‹å¦‚ã€Œåå…«ã€è®ºå››å‰ç¥èƒ½ç ´æ ¼ã€ã€‚
    ç”¨å‰ç»æ–­è¨€ä¿ç•™åºå·åœ¨æ¯ä¸ªå—å¼€å¤´ã€‚
    """
    parts = re.split(r"(?=\n[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+[ã€ï¼ã€‚])", text)
    return [p.strip() for p in parts if p.strip()]


def split_by_blank_line(text: str) -> list[str]:
    """
    æŒ‰ç©ºè¡Œåˆ‡åˆ†ï¼Œå¹¶å°†ã€Œå››å­—è¯€ + åŸæ³¨ + ä»»æ°æ›°ã€åˆå¹¶ä¸ºä¸€ä¸ªè¯­ä¹‰å—ï¼ˆæ»´å¤©é«“ï¼‰ã€‚

    æ»´å¤©é«“ç»“æ„ï¼š
        æ”¯ç¥åªä»¥å†²ä¸ºé‡ï¼Œåˆ‘ä¸ç©¿å…®åŠ¨ä¸åŠ¨ã€‚   â† å››å­—è¯€ï¼ˆçŸ­ï¼Œ<100å­—ï¼‰
        [ç©ºè¡Œ]
        åŸæ³¨ï¼šå†²è€…å¿…æ˜¯ç›¸å…‹...              â† åŸæ³¨æ®µè½
        [ç©ºè¡Œ]
        ä»»æ°æ›°ï¼š...                        â† ä»»æ³¨æ®µè½
        [ç©ºè¡Œ]
        ä¸‹ä¸€ä¸ªå››å­—è¯€...                    â† æ–°è¯­ä¹‰å—èµ·ç‚¹

    åˆå¹¶è§„åˆ™ï¼š
        - é‡åˆ°çŸ­æ®µè½ï¼ˆâ‰¤100å­—ï¼Œä¸ä»¥"åŸæ³¨"/"ä»»æ°æ›°"å¼€å¤´ï¼‰â†’ è§†ä¸ºå››å­—è¯€ï¼Œ
          ä¸å…¶åçš„æ®µè½åˆå¹¶ï¼Œç›´åˆ°ä¸‹ä¸€ä¸ªå››å­—è¯€å‡ºç°æˆ–åˆå¹¶ä½“è¶…è¿‡ MAX_CHUNK_CHARSã€‚
    """
    paragraphs = [p.strip() for p in re.split(r"\n\n+", text) if p.strip()]

    merged: list[str] = []
    i = 0
    while i < len(paragraphs):
        para = paragraphs[i]
        # åˆ¤æ–­æ˜¯å¦ä¸ºã€Œå››å­—è¯€ã€ï¼šçŸ­æ®µã€ä¸ä»¥åŸæ³¨/ä»»æ°æ›°å¼€å¤´
        is_verse = (
            len(para) <= 100
            and not para.startswith("åŸæ³¨")
            and not para.startswith("ä»»æ°æ›°")
            and not para.startswith("â—")
            and not para.startswith("ã€")
        )
        if is_verse and i + 1 < len(paragraphs):
            # åˆå¹¶åç»­æ®µè½ç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªå››å­—è¯€æˆ–è¶…å‡ºå¤§å°é™åˆ¶
            combined = para
            j = i + 1
            while j < len(paragraphs):
                next_para = paragraphs[j]
                next_is_verse = (
                    len(next_para) <= 100
                    and not next_para.startswith("åŸæ³¨")
                    and not next_para.startswith("ä»»æ°æ›°")
                    and not next_para.startswith("â—")
                    and not next_para.startswith("ã€")
                )
                # å·²æœ‰å®è´¨å†…å®¹ï¼ˆ>100å­—ï¼‰æ—¶é‡åˆ°æ–°å››å­—è¯€ â†’ åœæ­¢åˆå¹¶
                if next_is_verse and len(combined) > 100:
                    break
                combined = combined + "\n\n" + next_para
                j += 1
                if len(combined) >= MAX_CHUNK_CHARS:
                    break
            merged.append(combined)
            i = j
        else:
            merged.append(para)
            i += 1

    return merged


# â”€â”€ äºŒæ¬¡åˆ†å‰² + åˆå¹¶æå°å— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def secondary_split(chunks: list[str]) -> list[str]:
    """
    è¿‡å¤§å—ï¼ˆ>1.5Ã—MAX_CHUNK_CHARSï¼‰â†’ RecursiveCharacterTextSplitter äºŒæ¬¡åˆ‡ã€‚
    æå°å—ï¼ˆ<MIN_CHUNK_CHARSï¼‰â†’ åˆå¹¶åˆ°ä¸‹ä¸€å—ï¼Œé¿å…å­¤ç«‹ç¢ç‰‡ã€‚
    """
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_core.documents import Document

    secondary = RecursiveCharacterTextSplitter(
        chunk_size=MAX_CHUNK_CHARS,
        chunk_overlap=100,
        length_function=len,
        separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ã€", " ", ""],
    )

    expanded: list[str] = []
    for chunk in chunks:
        if len(chunk) > MAX_CHUNK_CHARS * 1.5:
            sub_docs = secondary.split_documents([Document(page_content=chunk)])
            expanded.extend(s.page_content for s in sub_docs)
        else:
            expanded.append(chunk)

    # åˆå¹¶æå°å—
    final: list[str] = []
    buffer = ""
    for chunk in expanded:
        if len(chunk) < MIN_CHUNK_CHARS:
            buffer += (" " if buffer else "") + chunk
        else:
            if buffer:
                final.append(buffer.strip())
                buffer = ""
            final.append(chunk)
    if buffer:
        if final:
            final[-1] = final[-1] + " " + buffer.strip()
        else:
            final.append(buffer.strip())

    return final


# â”€â”€ ç»Ÿè®¡è¾…åŠ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def chunk_stats(chunks: list[str]) -> dict:
    sizes = sorted(len(c) for c in chunks)
    n = len(sizes)
    return {
        "count": n,
        "min": sizes[0],
        "p25": sizes[n // 4],
        "median": sizes[n // 2],
        "p75": sizes[3 * n // 4],
        "max": sizes[-1],
        "mean": int(sum(sizes) / n),
    }


# â”€â”€ ä¸»æµç¨‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    import os
    import shutil
    from langchain_core.documents import Document
    from langchain_chroma import Chroma
    from langchain_huggingface import HuggingFaceEmbeddings

    project_root = Path(__file__).parent.parent
    os.chdir(project_root)

    # â”€â”€ åŠ è½½åµŒå…¥æ¨¡å‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"Loading embedding model: {EMBEDDING_MODEL}")
    emb = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},  # BGE æ¨èå½’ä¸€åŒ–
    )
    print("  âœ“ Embedding model loaded\n")

    all_docs: list[Document] = []

    # â”€â”€ å„ä¹¦å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for book_cfg in BOOKS:
        pdf_path  = book_cfg["path"]
        label     = book_cfg["label"]
        stype     = book_cfg["splitter"]

        print(f"{'â”€'*55}")
        print(f"ğŸ“–  {label}  ({pdf_path})")

        full_text = extract_full_text(pdf_path)
        print(f"    Total chars : {len(full_text):,}")

        if stype == "circle_marker":
            raw_chunks = split_by_circle_marker(full_text)
        elif stype == "blank_line":
            raw_chunks = split_by_blank_line(full_text)
        elif stype == "chapter_number":
            raw_chunks = split_by_chapter_number(full_text)
        else:
            raise ValueError(f"Unknown splitter type: {stype}")

        print(f"    After semantic split : {len(raw_chunks)} chunks")

        final_chunks = secondary_split(raw_chunks)
        st = chunk_stats(final_chunks)
        print(f"    After secondary split: {st['count']} chunks")
        print(f"    Size stats  : min={st['min']}  p25={st['p25']}  "
              f"median={st['median']}  p75={st['p75']}  max={st['max']}  mean={st['mean']}")

        for chunk_text in final_chunks:
            all_docs.append(Document(
                page_content=chunk_text,
                metadata={"source": label, "book": label},
            ))

    print(f"\n{'='*55}")
    print(f"Total documents to index: {len(all_docs)}")

    # â”€â”€ æ¸…ç©ºå¹¶é‡å»º ChromaDB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    chroma_path = Path(CHROMA_DIR)
    if chroma_path.exists():
        print(f"Removing existing index at {CHROMA_DIR} ...")
        shutil.rmtree(chroma_path)

    print(f"Building Chroma index at: {CHROMA_DIR}")
    print("(This may take a minute while encoding all documents...)")

    vectorstore = Chroma.from_documents(
        documents=all_docs,
        embedding=emb,
        persist_directory=CHROMA_DIR,
        collection_name=COLLECTION_NAME,
    )

    count = vectorstore._collection.count()
    print(f"\nâœ“ Indexed {count} chunks  â†’  {CHROMA_DIR}")

    # â”€â”€ åˆ†ä¹¦ç»Ÿè®¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nPer-book chunk counts:")
    for book_cfg in BOOKS:
        label = book_cfg["label"]
        res = vectorstore.get(where={"book": label})
        print(f"  {label}: {len(res['ids'])} chunks")

    print("\nâœ…  Done! Use configs/rag/v2/*.yaml to benchmark against this index.")


if __name__ == "__main__":
    main()
